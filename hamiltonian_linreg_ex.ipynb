{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write initial code for simple linear regression:\n",
    "\n",
    "$$ y_i \\sim N(\\beta X_i, 1/\\phi) $$\n",
    "$$ \\beta \\sim N(0, \\tau) $$\n",
    "$$ \\phi \\sim Gam(a, b) $$\n",
    "Let $ \\theta = (\\beta, \\phi) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $\\theta_0$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "true_beta = stats.norm.rvs(1)\n",
    "true_phi = stats.gamma.rvs(1)\n",
    "\n",
    "x = stats.norm.rvs(size=20)\n",
    "y = stats.norm.rvs(true_beta*x, true_phi)\n",
    "\n",
    "beta0 = stats.norm.rvs(1)\n",
    "phi0 = stats.gamma.rvs(4,4)\n",
    "theta0 = np.array((beta0, phi0))\n",
    "\n",
    "#Set hyperparameters\n",
    "tau = 1/10\n",
    "a = 1/2\n",
    "b= 1/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leapfrog(theta, r, eps):\n",
    "    n = y.shape[0]\n",
    "    #gradients are specific to this example\n",
    "    gradients = np.array((-tau * theta[0] + x.dot(y) - theta[0] * x.dot(x), (n/2 + a/2 -1)*1/theta[1] - b - 1/2* sum((y-theta[0]*x)**2)))\n",
    "    r_upd = r + eps/2 * (gradients)\n",
    "    theta_upd = theta + eps * r_upd\n",
    "    r_upd = r_upd + eps/2 * (gradients)\n",
    "    return theta_upd, r_upd\n",
    "    \n",
    "        \n",
    "def log_joint(y, x, theta):\n",
    "    n = y.shape[0]\n",
    "    #seems like this can be up to a normalizing constant so that's what i did, but otherwise that could be the issue\n",
    "    return (n/2 + a/2 -1)*np.log(theta[1]) - theta[1]*b -1/2 *(tau*theta[0]**2 + theta[1]*sum((y-theta[0]*x)**2))\n",
    "\n",
    "def hamilt_mc(theta0, y,x, eps, L, M):\n",
    "    theta_m = np.zeros((M,2))\n",
    "    theta_m[0,:] = theta0\n",
    "    accept_b = 0\n",
    "    accept_p = 0\n",
    "    for m in range(1,M):\n",
    "        r0 = stats.norm.rvs(size=2)\n",
    "        theta_m[m,:] = theta_m[m-1,:]\n",
    "        theta_tilde = theta_m[m-1,:]\n",
    "        r_tilde = r0\n",
    "        for i in range(L):\n",
    "            theta_tilde, r_tilde = leapfrog(theta_tilde, r_tilde, eps) \n",
    "        \n",
    "        #need to deal with negative proposal values for phi (theta[1]) - doubt this is the correct way\n",
    "        #seems to not be going in here .. so maybe it's fine\n",
    "        if theta_tilde[1] < 0:\n",
    "            print(\"reject\")\n",
    "            theta_tilde[1] = theta_m[m-1,1]\n",
    "            accept_p -=1\n",
    "        alpha = min(1, np.exp(log_joint(y,x, theta_tilde) - 1/2 * r_tilde.dot(r_tilde))/np.exp(log_joint(y,x, theta_m[m-1,:]) -1/2*r0.dot(r0)))\n",
    "        u = np.random.uniform()\n",
    "        if alpha > u:\n",
    "            theta_m[m,:] = theta_tilde\n",
    "            r_m = - r_tilde #confused why you save this each time since the algorithm has you repropose an r0 each iteration\n",
    "            accept_b +=1\n",
    "            accept_p +=1\n",
    "    return(theta_m, accept_b, accept_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth (0.7614318131175473, 0.6032127880106088)\n",
      "we find [ 0.71365077  8.96742214]\n",
      "acceptance (beta then phi) 0.487 0.487\n"
     ]
    }
   ],
   "source": [
    "no_iters = 1000\n",
    "results,acc_b, acc_p = hamilt_mc(theta0, y,x, 1/100, 100, no_iters)\n",
    "\n",
    "#phi is off - but this seems to be working for beta\n",
    "print(\"truth\", (true_beta, true_phi))\n",
    "print(\"we find\", results[99,:])\n",
    "print(\"acceptance (beta then phi)\", acc_b/no_iters, acc_p/no_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for 250-dimensional multivariate normal (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Known precision matrix A - df=250 with identity scale\n",
    "A = stats.wishart.rvs(df=250, scale= np.eye(250))\n",
    "#target distribution is zero-mean 250-dimensional multivariate normal with known precision\n",
    "\n",
    "theta0 = stats.norm.rvs(size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def leapfrog2(theta, A, r, eps):\n",
    "    #gradients are specific to this example\n",
    "    gradients = A.dot(theta)\n",
    "    r_upd = r + eps/2 * (gradients)\n",
    "    theta_upd = theta + eps * r_upd\n",
    "    r_upd = r_upd + eps/2 * (gradients)\n",
    "    return theta_upd, r_upd\n",
    "    \n",
    "        \n",
    "def log_joint2(theta, A):\n",
    "    #seems like this can be up to a normalizing constant so that's what i did, but otherwise that could be the issue\n",
    "    return -1/2 * theta.T.dot(A).dot(theta)\n",
    "\n",
    "def hamilt_mc2(theta0, A, eps, L, M):\n",
    "    no_par = theta0.shape[0]\n",
    "    theta_m = np.zeros((M,no_par))\n",
    "    theta_m[0,:] = theta0\n",
    "    accept_th = 0\n",
    "    for m in range(1,M):\n",
    "        r0 = stats.norm.rvs(size=no_par)\n",
    "        theta_m[m,:] = theta_m[m-1,:]\n",
    "        theta_tilde = theta_m[m-1,:]\n",
    "        r_tilde = r0\n",
    "        #Leapfrog\n",
    "        for i in range(L):\n",
    "            theta_tilde, r_tilde = leapfrog2(theta_tilde, A, r_tilde, eps) #keeps pushing phi towards negative\n",
    "        \n",
    "        alpha = min(1, np.exp(log_joint2(theta_tilde,A) - 1/2 * r_tilde.dot(r_tilde))/np.exp(log_joint2(theta_m[m-1,:], A) -1/2*r0.dot(r0)))\n",
    "        u = np.random.uniform()\n",
    "        if alpha > u:\n",
    "            accept_th += 1\n",
    "            theta_m[m,:] = theta_tilde\n",
    "            r_m = - r_tilde #confused why you save this each time since the algorithm has you repropose an r0 each iteration\n",
    "    return(theta_m, accept_th)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elorenzi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "results, acc= hamilt_mc2(theta0, A, 1/10, 10, 1000)\n",
    "\n",
    "print(acc) #getting weird runtime warning - may need to update this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
