{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 663 Final Project: NUTS Algorithm\n",
    "## Stephanie Brown, Liz Lorenzi, Jody Wortman\n",
    "### April 6, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract: Our project aims to implement and explore the No-U-Turn Sampler (NUTS), an algorithm for adaptively setting path lengths in Hamiltonian Monte Carlo. Hamiltonian Monte Carlo (HMC) is an MCMC algorithm that uses first-order gradient information to enable faster convergence by avoiding the random walk behavior and sensitivity to correlated parameters.  Specifically, NUTS uses a recursive algorithm which eliminates the need to manually tune parameters for step size and number of steps.  We implement this algorithm and compare its posterior accuracy and computational efficiency to other MCMC methods, such as Metropolis-Hastings, Gibbs samplers, and Hamiltonian MCMC. We show examples of how the tuning parameters from HMC can result in poor posterior estimations, and how the NUTS recursive tuning results in similar or better estimation without the need for manual tuning. In addition, we implement the Efficient NUTS algorithm as well as the NUTS with Dual Averaging, to explore the speed ups available for this algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a growing field of Bayesian statistics and with the increasing use of Bayesian hierarchical models, valid methods for posterior estimation are critical. Because posterior distributions for some models are impossible to calculate analytically, researchers and analysts rely on Markov Chain Monte Carlo (MCMC) when tractable posteriors are unavailable. Commonly used MCMC methods are symmetric Metropolis algorithms and Gibbs samplers. Both of these methods are simple but can take so long to converge that they are not useful. While Hamiltonian Monte Carlo (HMC) addresses some of these issues via an approach that uses a set auxiliary variables, the approach necessitates user input of tuning parameters. Because performance is dependent on these parameters, HMC is not as widely used as it could be given its benefits. This issue is addressed by an algorithm called the No U-Turn Sampler (NUTS) proposed by Hoffman and Gelman \\citeyear{homan2014no}. NUTS adaptively sets these parameters so that they do not require user input. \\\\\n",
    "In this paper, we review the background of MCMC and HMC, motivating the adaptation. We then introduce NUTS and outline its advantages. We implement the algorithm in various stages of complexity and efficiency, compare it to other common MCMC methods, explore its strengths and weaknesses, and discuss its benefits to the Bayesian statistics community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMC methods simulate from a posterior distribution through drawing correlated samples--if the algorithm is correct and appropriate, these will converge to the target distribution \\cite{neal}. These methods differ from each other--one common one is Gibbs sampling \\cite{geman}, which updates conditional distributions until they all converge. However, this approach requires deriving the conditional distributions, which may not always be possible. Another approach is random walk Metropolis \\cite{metropolis}, which uses likelihood ratios to either move to a new point in the posterior distribution or remain in the same place. Both methods may take a very long time to converge, however, as they both explore the parameter space in a stochastic, not necessarily efficient manner. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamiltonian Monte Carlo (HMC, also called Hybrid Monte Carlo) \\cite{duane, neal} is a method which uses auxiliary variables in the Monte Carlo proposals. The method has a physical motivation, where each of the auxiliary variables $r_d$ correspond to the momentum of each model variable $\\theta_d$. The algorithm uses direction and gradient information to update the values. Hoffman and Gelman outline a version of HMC in their paper where a leapfrog function is used to update $r$ and $\\theta$ using the leapfrog integrator. The parameter proposals are then accepted or rejected using a Metropolis algorithm \\cite{metropolis}. This method can work very well and converge faster than random walk Metropolis, but its degree of success is highly dependent on choosing both the step size $\\epsilon$ and the number of steps $L$. Poor choices have varying consequences from inaccuracy to lack of convergence, but they generally lead to a slow but ergodic chain \\cite{homan2014no}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS Algorithm and Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the issues caused by poor choices of $\\epsilon$ and $L$ (though mostly $L$), Hoffman and Gelman's paper develops a method to adaptively select these parameters. The No-U-Turn Sampler eliminates the need for select the number of steps, and NUTS with dual averaging allows $\\epsilon$ to also be adaptively set. The paper outlines the NUTS algorithm, then dual averaging. First, there is a naive approach, then an efficient version. We implement both and test their performance against each other and a standard Gibbs sampler and random walk Metropolis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gamma\n",
    "\n",
    "n=50\n",
    "\n",
    "true_beta = np.transpose(stats.norm.rvs(loc=0,scale=1,size=2))\n",
    "true_phi = stats.gamma.rvs(a=3,scale=1/2,size=1)\n",
    "\n",
    "x = np.transpose(np.array([np.ones(n),stats.norm.rvs(loc=0,scale=1,size=n)]))\n",
    "y = np.random.normal(x.dot(true_beta), np.sqrt(1/true_phi))\n",
    "\n",
    "\n",
    "beta0 = stats.norm.rvs(loc=0,scale=1,size=2)\n",
    "phi0 = stats.gamma.rvs(a=3,scale=1/2,size=1)\n",
    "theta0 = np.hstack([beta0, phi0])\n",
    "\n",
    "#Set hyperparameters\n",
    "a = 3.0\n",
    "b= 2.0\n",
    "\n",
    "print(true_beta,true_phi)\n",
    "print(theta0)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(beta0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leapfrog(y,x,theta, r, eps):\n",
    "    n = y.shape[0]\n",
    "    #gradients = np.hstack([theta[2]*(sum(np.transpose(x).dot(y))-theta[0:2]*sum(np.transpose(x).dot(x)))-theta[0:2],(.5*n+a-1)/theta[2]-b-.5*sum((y-x.dot(theta[0:2]))*(y-x.dot(theta[0:2])))])\n",
    "    gradients = np.hstack([theta[2]*(np.transpose(y).dot(x)-np.transpose(x).dot(x).dot(theta[0:2]))-.01*theta[0:2],(.5*n+a-1)/theta[2]-b-0.5*np.transpose((y-x.dot(theta[0:2]))).dot(y-x.dot(theta[0:2]))])\n",
    "    r_upd = r + eps/2 * (gradients)\n",
    "    theta_upd = theta + eps * r_upd\n",
    "    gradients = np.hstack([theta_upd[2]*(np.transpose(y).dot(x)-np.transpose(x).dot(x).dot(theta_upd[0:2]))-.01*theta_upd[0:2],(.5*n+a-1)/theta[2]-b-0.5*np.transpose((y-x.dot(theta_upd[0:2]))).dot(y-x.dot(theta_upd[0:2]))])\n",
    "    r_upd = r_upd + eps/2 * (gradients)\n",
    "    return theta_upd, r_upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_joint(y, x, theta): \n",
    "    n = y.shape[0]\n",
    "    return sum(norm.logpdf(y,loc=x.dot(theta[0:2]), scale=1/np.sqrt(theta[2])))+norm.logpdf(theta[0],loc=0,scale=1/np.sqrt(.01))+norm.logpdf(theta[1],loc=0,scale=1/np.sqrt(.01)) +gamma.logpdf(theta[2], a, scale=1/b)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hamilt_mc(theta0, y,x, eps, L, M):\n",
    "    theta_m = np.zeros((M,3))\n",
    "    theta_m[0,:] = theta0\n",
    "    accept_b = 0\n",
    "    accept_p = 0\n",
    "    for m in range(1,M):\n",
    "        r0 = stats.norm.rvs(size=3)\n",
    "        theta_m[m,:] = theta_m[m-1,:]\n",
    "        theta_tilde = theta_m[m-1,:]\n",
    "        r_tilde = r0\n",
    "        for i in range(L):\n",
    "            theta_tilde, r_tilde = leapfrog(y,x,theta_tilde, r_tilde, eps) \n",
    "        if theta_tilde[2] < 0.0:\n",
    "            print(\"reject\")\n",
    "            theta_tilde[2] = theta_m[m-1,2]\n",
    "        alpha = min(1, np.exp(log_joint(y,x,theta_tilde)-(1/2)*r_tilde.dot(r_tilde))/np.exp(log_joint(y,x,theta_m[m-1,:])-1/2*r0.dot(r0)))\n",
    "        u = np.random.uniform()\n",
    "        if alpha > u:\n",
    "            theta_m[m,:] = theta_tilde\n",
    "            r_m =-r_tilde #confused why you save this each time since the algorithm has you repropose an r0 each iteration\n",
    "    return(theta_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 10000\n",
    "eps = .001\n",
    "L= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = hamilt_mc(theta0, y,x, eps, M)\n",
    "print(\"truth\", (true_beta, true_phi))\n",
    "beta0_found = np.mean(results[round(M/5):(M-1),0])\n",
    "beta1_found = np.mean(results[round(M/5):(M-1),1])\n",
    "phi_found = np.mean(results[round(M/5):(M-1),2])\n",
    "print(\"Mean after burn in we find\",beta0_found,beta1_found ,phi_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5dfd950918de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbeta0s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbeta1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mphis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "beta0s = results[round(4*M/5):(M-1),0]\n",
    "beta1s = results[round(4*M/5):(M-1),1]\n",
    "phis = results[round(4*M/5):(M-1),2]\n",
    "\n",
    "y_sim = stats.norm.rvs(loc=x.dot(betas_found),scale=1/np.sqrt(phi_found))\n",
    "#y_sim = stats.norm.rvs(loc=x*beta_found,scale=phi_found)\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(beta0s, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_beta[0], c=\"r\", label=\"True Beta0\")\n",
    "plt.title(\"Draws of Beta_0\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "fig1.savefig('beta0s_hmc.png')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(beta1s, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_beta[1], c=\"r\", label=\"True Beta1\")\n",
    "plt.title(\"Draws of Beta_1\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "fig2.savefig('beta1s_hmc.png')\n",
    "\n",
    "fig3 = plt.figure()\n",
    "plt.plot(phis, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_phi, c=\"r\", label=\"True Phi\")\n",
    "plt.title(\"Draws of Phi\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "fig3.savefig('phis_hmc.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No U-Turn Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree(y,x,theta, r, u, v, j, eps):\n",
    "    triangle_max = 1000 #recommend value pg 1359\n",
    "    if(j==0):\n",
    "        #base case, take one leapfrog step in direction v\n",
    "        theta_prime,r_prime = leapfrog(y,x,theta,r,v*eps)\n",
    "        if(np.linalg.norm(u) < np.exp(log_joint(y,x,theta_prime)-(1/2)*r_prime.dot(r_prime))):\n",
    "            C_theta_prime = theta_prime\n",
    "            C_r_prime = r_prime\n",
    "        else:\n",
    "            C_theta_prime = np.array([])\n",
    "            C_r_prime = np.array([])\n",
    "        if(log_joint(y,x,theta_prime)-(1/2)*r_prime.dot(r_prime) > u-triangle_max):\n",
    "            s_prime = 1\n",
    "        else:\n",
    "            s_prime = 0\n",
    "        return theta_prime,r_prime,theta_prime,r_prime,C_theta_prime,C_r_prime,s_prime    \n",
    "    else:\n",
    "        #recursion-build the left and right subtrees\n",
    "        theta_minus,r_minus,theta_plus,r_plus,C_theta_prime,C_r_prime,s_prime = BuildTree(y,x,theta,r,u,v,j-1,eps)\n",
    "        if(v == -1):\n",
    "            theta_minus,r_minus,dash1,dash2,C_theta_primep,C_r_primep,s_primep = BuildTree(y,x,theta_minus,r_minus,u,v,j-1,eps)\n",
    "        else:\n",
    "            dash1,dash2,theta_plus,r_plus,C_theta_primep,C_r_primep,s_primep = BuildTree(y,x,theta_plus,r_plus,u,v,j-1,eps)\n",
    "        if((theta_plus-theta_minus).dot(r_minus) > 0 and (theta_plus-theta_minus).dot(r_plus) > 0):  \n",
    "            s_prime = s_prime*s_primep  \n",
    "        else:\n",
    "            s_prime = 0  \n",
    "        C_theta_prime = np.hstack([C_theta_prime,C_theta_primep])\n",
    "        C_r_prime = np.hstack([C_r_prime,C_r_primep])\n",
    "        return theta_minus,r_minus,theta_plus,r_plus,C_theta_prime,C_r_prime,s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@jit\n",
    "def nuts(theta0, y,x, eps, M):\n",
    "    theta_m = np.zeros((M,3))\n",
    "    theta_m[0,:] = theta0\n",
    "    for m in range(1,M):\n",
    "        #print('M: ',m,' Theta: ',theta_m[m-1,:])\n",
    "        r0 = stats.norm.rvs(size=3)\n",
    "        u = np.random.uniform(low=0,high=np.exp(log_joint(y,x,theta_m[m-1,:])-(1/2)*r0.dot(r0)))\n",
    "        theta_minus = theta_m[m-1,:]\n",
    "        theta_plus = theta_m[m-1,:]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j=0\n",
    "        C_theta = theta_m[m-1,:]\n",
    "        C_r = r0\n",
    "        s=1\n",
    "        while(s==1):\n",
    "            v_j = np.random.choice([-1,1])\n",
    "            if(v_j==-1):\n",
    "                theta_minus,r_minus,dash1,dash2,C_theta_prime,C_r_prime,s_prime = BuildTree(y,x,theta_minus,r_minus,u,v_j,j,eps)\n",
    "            else:\n",
    "                dash1,dash2,theta_plus,r_plus,C_theta_prime,C_r_prime,s_prime = BuildTree(y,x,theta_plus,r_plus,u,v_j,j,eps)\n",
    "            if(s_prime == 1):\n",
    "                C_theta = np.hstack([C_theta,C_theta_prime])\n",
    "                C_r = np.hstack([C_r,C_r_prime]) \n",
    "            if((theta_plus-theta_minus).dot(r_minus) >= 0 and (theta_plus-theta_minus).dot(r_plus) >= 0):\n",
    "                s = s_prime\n",
    "            else:\n",
    "                s = 0\n",
    "            j = j+1     \n",
    "        index = np.random.randint(len(C_theta))\n",
    "        if(index%3 ==1):\n",
    "            index = index-1   \n",
    "        if(index%3 ==2):\n",
    "            index = index-2    \n",
    "        theta_m[m,:] = [C_theta[index],C_theta[index+1],C_theta[index+2]] \n",
    "    return(theta_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 500\n",
    "eps = .003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5bec75d9a8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'results = hamilt_mc(theta0, y,x, eps, M)\\nprint(\"truth\", (true_beta, true_phi))\\nbeta0_found = np.mean(results[round(4*M/5):(M-1),0])\\nbeta1_found = np.mean(results[round(4*M/5):(M-1),1])\\nphi_found = np.mean(results[round(4*M/5):(M-1),2])\\nprint(\"Mean after burn in we find\",beta0_found,beta1_found ,phi_found)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/elorenzi/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/elorenzi/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/elorenzi/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta0' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = hamilt_mc(theta0, y,x, eps, M)\n",
    "print(\"truth\", (true_beta, true_phi))\n",
    "beta0_found = np.mean(results[round(4*M/5):(M-1),0])\n",
    "beta1_found = np.mean(results[round(4*M/5):(M-1),1])\n",
    "phi_found = np.mean(results[round(4*M/5):(M-1),2])\n",
    "print(\"Mean after burn in we find\",beta0_found,beta1_found ,phi_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "beta0s = results[round(1*M/5):(M-1),0]\n",
    "beta1s = results[round(1*M/5):(M-1),1]\n",
    "phis = results[round(1*M/5):(M-1),2]\n",
    "\n",
    "y_sim = stats.norm.rvs(loc=x.dot(betas_found),scale=1/np.sqrt(phi_found))\n",
    "#y_sim = stats.norm.rvs(loc=x*beta_found,scale=phi_found)\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(beta0s, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_beta[0], c=\"r\", label=\"True Beta0\")\n",
    "plt.title(\"Draws of Beta_0\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "fig1.savefig('beta0s_nuts.png')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(beta1s, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_beta[1], c=\"r\", label=\"True Beta1\")\n",
    "plt.title(\"Draws of Beta_1\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "fig2.savefig('beta1s_nuts.png')\n",
    "\n",
    "fig3 = plt.figure()\n",
    "plt.plot(phis, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_phi, c=\"r\", label=\"True Phi\")\n",
    "plt.title(\"Draws of Phi\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "fig3.savefig('phis_nuts.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of NUTS addresses key efficiency issues in the Hamiltonian Monte Carlo method. HMC is limited by its need of tuning of the step sizes ($\\epsilon$) and number of steps($L$), typically needing multiple runs to find the correct parameter values. In addition, Hoffman and Gelman provide additional improvements in the NUTS algorithm to improve the efficiency. The first is Efficient No-U-Turn Sampler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient No-U-Turn Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original NUTS algorithm takes $2^j -1$ evaluations of the log likelihood and its gradient, where $j$ is the number of times we call BuildTree (see Appendix for more information on algorithms). In addition, there are $O(2^j)$ operations to determine when to stop doubling. When implemented, the most costly computational aspect of the original NUTS algorithm is the computation of the log joint distribution and its gradient. This results in an algorithm that has similar computational complexity to the original HMC. Another computationally costly aspect of the original NUTS is the storage of the $2^j$ position and momentum vectors, requiring a lot of memory.  Efficient NUTS aims to to solve these issues in the following ways. First, we change the transition kernel to produce larger jumps than the simple uniform sampling. This kernel has a memory-efficient implementation, reducing the number of position and momentum vectors stored to $O(j)$ instead of $O(2^j)$ Second, they aim to find a stopping criterion in the final doubling iteration to avoid wasting computation to build a set that won't be used. This is done by breaking out of the recursion when they encounter a zero for the stop indicator $s$.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree(theta,A, r, u, v, j, eps):\n",
    "    triangle_max = 1000 #recommend value pg 1359\n",
    "    if(j==0):\n",
    "        #base case, take one leapfrog step in direction v\n",
    "        theta_prime,r_prime = leapfrog(theta,A,r,v*eps)\n",
    "        if(u <= np.exp(log_joint(theta_prime,A)-(1/2)*r_prime.dot(r_prime))):\n",
    "            n_prime = 1\n",
    "        else:\n",
    "            n_prime = 0\n",
    "        if(log_joint(theta_prime,A)-(1/2)*r_prime.dot(r_prime) > u-triangle_max):\n",
    "            s_prime = 1\n",
    "        else:\n",
    "            s_prime = 0\n",
    "        return theta_prime,r_prime,theta_prime,r_prime,theta_prime,n_prime,s_prime    \n",
    "    else:\n",
    "        #recursion-build the left and right subtrees\n",
    "        theta_minus,r_minus,theta_plus,r_plus,theta_prime,n_prime,s_prime = BuildTree(theta,A,r,u,v,j-1,eps)\n",
    "        if(s_prime==1):\n",
    "            if(v == -1):\n",
    "                theta_minus,r_minus,dash1,dash2,theta_primep,n_primep,s_primep = BuildTree(theta_minus,A,r_minus,u,v,j-1,eps)\n",
    "            else:\n",
    "                dash1,dash2,theta_plus,r_plus,theta_primep,n_primep,s_primep = BuildTree(theta_plus,A,r_plus,u,v,j-1,eps)\n",
    "            #if(n_prime+n_primep==0):\n",
    "                #print('Ahhhh cant divide by zero:',n_prime,n_primep)\n",
    "                #p=0.99\n",
    "            #else:    \n",
    "            p = np.exp(np.log(n_primep)-np.log(n_prime+n_primep))\n",
    "            unif = np.random.uniform()\n",
    "            if(p>u):\n",
    "                theta_prime = theta_primep\n",
    "            if((theta_plus-theta_minus).dot(r_minus) >= 0 and (theta_plus-theta_minus).dot(r_plus) >= 0):  \n",
    "                s_prime = s_primep  \n",
    "            else:\n",
    "                s_prime = 0  \n",
    "            n_prime = n_prime+n_primep\n",
    "    return theta_minus,r_minus,theta_plus,r_plus,theta_prime,n_prime,s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nuts_eff(theta0, y,x, eps, M):\n",
    "    theta_m = np.zeros((M,3))\n",
    "    theta_m[0,:] = theta0\n",
    "    for m in range(1,M):\n",
    "        #print('M: ',m,' Theta: ',theta_m[m-1,:])\n",
    "        r0 = stats.norm.rvs(size=3)\n",
    "        u = np.random.uniform(low=0,high=np.exp(log_joint(y,x,theta_m[m-1,:])-(1/2)*r0.dot(r0)))\n",
    "        theta_minus = theta_m[m-1,:]\n",
    "        theta_plus = theta_m[m-1,:]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j=0\n",
    "        theta_m[m,:] = theta_m[m-1,:]\n",
    "        n = 1\n",
    "        s=1\n",
    "        while(s==1):\n",
    "            v_j = np.random.choice([-1,1])\n",
    "            if(v_j==-1):\n",
    "                theta_minus,r_minus,dash1,dash2,theta_prime,n_prime,s_prime = BuildTree(y,x,theta_minus,r_minus,u,v_j,j,eps)\n",
    "            else:\n",
    "                dash1,dash2,theta_plus,r_plus,theta_prime,n_prime,s_prime = BuildTree(y,x,theta_plus,r_plus,u,v_j,j,eps)\n",
    "            if(s_prime == 1):\n",
    "                p = min(1,n_prime/n)\n",
    "                unif = np.random.uniform()\n",
    "                if(p>u):\n",
    "                    theta_m[m,:] = theta_prime\n",
    "            n = n+n_prime\n",
    "            if((theta_plus-theta_minus).dot(r_minus) >= 0 and (theta_plus-theta_minus).dot(r_plus) >= 0):\n",
    "                s = s_prime\n",
    "            else:\n",
    "                s = 0\n",
    "            j = j+1    \n",
    "    return(theta_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 1000\n",
    "eps = .005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = nuts_eff(theta0, y,x, eps, M)\n",
    "print(\"truth\", (true_beta, true_phi))\n",
    "beta0_found = np.mean(results[round(4*M/5):(M-1),0])\n",
    "beta1_found = np.mean(results[round(4*M/5):(M-1),1])\n",
    "phi_found = np.mean(results[round(4*M/5):(M-1),2])\n",
    "print(\"Mean after burn in we find\",beta0_found,beta1_found ,phi_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "beta0s = results[round(1*M/5):(M-1),0]\n",
    "beta1s = results[round(1*M/5):(M-1),1]\n",
    "phis = results[round(1*M/5):(M-1),2]\n",
    "\n",
    "y_sim = stats.norm.rvs(loc=x.dot(betas_found),scale=1/np.sqrt(phi_found))\n",
    "#y_sim = stats.norm.rvs(loc=x*beta_found,scale=phi_found)\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(beta0s, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_beta[0], c=\"r\", label=\"True Beta0\")\n",
    "plt.title(\"Draws of Beta_0\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "fig1.savefig('beta0s_nuts.png')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(beta1s, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_beta[1], c=\"r\", label=\"True Beta1\")\n",
    "plt.title(\"Draws of Beta_1\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "fig2.savefig('beta1s_nuts.png')\n",
    "\n",
    "fig3 = plt.figure()\n",
    "plt.plot(phis, c='b', marker=\"s\", label='Draws')\n",
    "plt.axhline(true_phi, c=\"r\", label=\"True Phi\")\n",
    "plt.title(\"Draws of Phi\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "fig3.savefig('phis_nuts.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No-U-Turn Sampler with Dual Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTS and Efficient NUTS addresses the issue of choosing the number of Leapfrog steps, $L$. NUTS with Dual Averaging aims to take this a step further to also adaptively tuning $\\epsilon$, the step size parameter. The Dual Averaging scheme is initially motivated by work of \\cite{robbins} where they propose a vanishing adaptation algorithm for MCMC using stochastic approximation. \n",
    "\n",
    "...may be removing this section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Speedups with Cython and Parrallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further improvements, we look to the power of Python and its ability to compile python code to C. Specifically, we use the Cython capabilities to convert our working functions to C files for even further optimality.  This speedup results in a $10\\%$ improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: Comparison to other MCMC methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
